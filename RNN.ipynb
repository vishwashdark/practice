{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'H': [1, 0, 0, 0, 0],\n",
       " 'E': [0, 1, 0, 0, 0],\n",
       " 'L': [0, 0, 0, 1, 0],\n",
       " 'O': [0, 0, 0, 0, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training a RNN model from scratch \n",
    "#1 create a encoding \n",
    "#one hot encoding\n",
    "def one_hot_encoding(input):\n",
    "    a={c:[1 if i==idx else 0 for i in range(len(input))]\n",
    "       for idx,c in enumerate(input)}\n",
    "    return a\n",
    "one_hot_encoding(\"HELLO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING TENSORFLOW (INBUILT MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 2.099057912826538\n",
      "Epoch 2/100, Loss: 1.9969029426574707\n",
      "Epoch 3/100, Loss: 1.9822297096252441\n",
      "Epoch 4/100, Loss: 2.064483404159546\n",
      "Epoch 5/100, Loss: 2.036527156829834\n",
      "Epoch 6/100, Loss: 2.0194830894470215\n",
      "Epoch 7/100, Loss: 1.8088974952697754\n",
      "Epoch 8/100, Loss: 2.0059268474578857\n",
      "Epoch 9/100, Loss: 1.9244847297668457\n",
      "Epoch 10/100, Loss: 1.814267873764038\n",
      "Epoch 11/100, Loss: 2.022545337677002\n",
      "Epoch 12/100, Loss: 1.8352160453796387\n",
      "Epoch 13/100, Loss: 1.6465314626693726\n",
      "Epoch 14/100, Loss: 1.7519997358322144\n",
      "Epoch 15/100, Loss: 1.5370376110076904\n",
      "Epoch 16/100, Loss: 1.6986160278320312\n",
      "Epoch 17/100, Loss: 1.6926426887512207\n",
      "Epoch 18/100, Loss: 1.6260029077529907\n",
      "Epoch 19/100, Loss: 1.2688549757003784\n",
      "Epoch 20/100, Loss: 0.6693180799484253\n",
      "Epoch 21/100, Loss: 1.196740746498108\n",
      "Epoch 22/100, Loss: 1.2172878980636597\n",
      "Epoch 23/100, Loss: 1.2028616666793823\n",
      "Epoch 24/100, Loss: 1.2921171188354492\n",
      "Epoch 25/100, Loss: 0.9955384135246277\n",
      "Epoch 26/100, Loss: 1.158467411994934\n",
      "Epoch 27/100, Loss: 0.23247072100639343\n",
      "Epoch 28/100, Loss: 1.0047584772109985\n",
      "Epoch 29/100, Loss: 1.123934030532837\n",
      "Epoch 30/100, Loss: 0.2009284347295761\n",
      "Epoch 31/100, Loss: 0.9455762505531311\n",
      "Epoch 32/100, Loss: 0.5388792157173157\n",
      "Epoch 33/100, Loss: 0.1461813747882843\n",
      "Epoch 34/100, Loss: 0.13733062148094177\n",
      "Epoch 35/100, Loss: 0.4595731496810913\n",
      "Epoch 36/100, Loss: 0.11944010108709335\n",
      "Epoch 37/100, Loss: 1.6315996646881104\n",
      "Epoch 38/100, Loss: 0.08026862889528275\n",
      "Epoch 39/100, Loss: 1.02290940284729\n",
      "Epoch 40/100, Loss: 1.011849045753479\n",
      "Epoch 41/100, Loss: 0.799298882484436\n",
      "Epoch 42/100, Loss: 0.7812254428863525\n",
      "Epoch 43/100, Loss: 0.2512291967868805\n",
      "Epoch 44/100, Loss: 0.0683286041021347\n",
      "Epoch 45/100, Loss: 1.0144866704940796\n",
      "Epoch 46/100, Loss: 0.19149132072925568\n",
      "Epoch 47/100, Loss: 0.04796746373176575\n",
      "Epoch 48/100, Loss: 1.0167218446731567\n",
      "Epoch 49/100, Loss: 0.7968046069145203\n",
      "Epoch 50/100, Loss: 0.12149852514266968\n",
      "Epoch 51/100, Loss: 1.0533474683761597\n",
      "Epoch 52/100, Loss: 0.04457691311836243\n",
      "Epoch 53/100, Loss: 0.12963828444480896\n",
      "Epoch 54/100, Loss: 0.041882604360580444\n",
      "Epoch 55/100, Loss: 0.03415338695049286\n",
      "Epoch 56/100, Loss: 1.0568077564239502\n",
      "Epoch 57/100, Loss: 0.7358222007751465\n",
      "Epoch 58/100, Loss: 0.09915456175804138\n",
      "Epoch 59/100, Loss: 0.7728247046470642\n",
      "Epoch 60/100, Loss: 0.7720309495925903\n",
      "Epoch 61/100, Loss: 0.07305154949426651\n",
      "Epoch 62/100, Loss: 0.031132718548178673\n",
      "Epoch 63/100, Loss: 1.060099720954895\n",
      "Epoch 64/100, Loss: 0.06274569779634476\n",
      "Epoch 65/100, Loss: 1.1005786657333374\n",
      "Epoch 66/100, Loss: 0.07332510501146317\n",
      "Epoch 67/100, Loss: 0.026067260652780533\n",
      "Epoch 68/100, Loss: 1.1451563835144043\n",
      "Epoch 69/100, Loss: 1.1527549028396606\n",
      "Epoch 70/100, Loss: 1.1894285678863525\n",
      "Epoch 71/100, Loss: 1.2603176832199097\n",
      "Epoch 72/100, Loss: 0.05889753997325897\n",
      "Epoch 73/100, Loss: 0.039223428815603256\n",
      "Epoch 74/100, Loss: 0.05372239649295807\n",
      "Epoch 75/100, Loss: 0.753627598285675\n",
      "Epoch 76/100, Loss: 0.751497745513916\n",
      "Epoch 77/100, Loss: 0.7435058355331421\n",
      "Epoch 78/100, Loss: 0.016216635704040527\n",
      "Epoch 79/100, Loss: 0.7642308473587036\n",
      "Epoch 80/100, Loss: 1.1820982694625854\n",
      "Epoch 81/100, Loss: 0.0184016190469265\n",
      "Epoch 82/100, Loss: 1.170000672340393\n",
      "Epoch 83/100, Loss: 1.183457851409912\n",
      "Epoch 84/100, Loss: 0.01633439026772976\n",
      "Epoch 85/100, Loss: 1.1528171300888062\n",
      "Epoch 86/100, Loss: 0.027930764481425285\n",
      "Epoch 87/100, Loss: 0.7486301064491272\n",
      "Epoch 88/100, Loss: 0.013288740068674088\n",
      "Epoch 89/100, Loss: 1.1933006048202515\n",
      "Epoch 90/100, Loss: 1.1813610792160034\n",
      "Epoch 91/100, Loss: 0.024265671148896217\n",
      "Epoch 92/100, Loss: 0.013785157352685928\n",
      "Epoch 93/100, Loss: 0.7042260766029358\n",
      "Epoch 94/100, Loss: 0.021045703440904617\n",
      "Epoch 95/100, Loss: 0.7242844700813293\n",
      "Epoch 96/100, Loss: 0.013356143608689308\n",
      "Epoch 97/100, Loss: 0.7416793704032898\n",
      "Epoch 98/100, Loss: 0.7117568254470825\n",
      "Epoch 99/100, Loss: 0.01864631101489067\n",
      "Epoch 100/100, Loss: 1.159439206123352\n",
      "helorlorlorlorlorlorl\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# One-hot encoding\n",
    "def one_hot_encoding(input_sequence):\n",
    "    unique_chars = sorted(set(input_sequence))\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "    return char_to_idx, idx_to_char\n",
    "\n",
    "# Prepare sequences\n",
    "def prepare_sequences(sequence, char_to_idx):\n",
    "    inputs = sequence[:-1]\n",
    "    targets = sequence[1:]\n",
    "    input_indices = [char_to_idx[char] for char in inputs]\n",
    "    target_indices = [char_to_idx[char] for char in targets]\n",
    "    return input_indices, target_indices\n",
    "\n",
    "# Build the RNN model using TensorFlow's pre-built SimpleRNN\n",
    "def build_rnn_model(vocab_size, embedding_dim, hidden_units):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
    "        tf.keras.layers.SimpleRNN(hidden_units, return_sequences=True, return_state=False),\n",
    "        tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Generate text\n",
    "def generate_text(model, start_char, char_to_idx, idx_to_char, length=20):\n",
    "    input_idx = np.array([char_to_idx[start_char]])\n",
    "    result = start_char\n",
    "\n",
    "    for _ in range(length):\n",
    "        predictions = model.predict(input_idx.reshape(1, -1), verbose=0)\n",
    "        predicted_idx = np.argmax(predictions[0, -1])\n",
    "        result += idx_to_char[predicted_idx]\n",
    "        input_idx = np.array([predicted_idx])\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_sequence = \"hello world\"\n",
    "    char_to_idx, idx_to_char = one_hot_encoding(input_sequence)\n",
    "    inputs, targets = prepare_sequences(input_sequence, char_to_idx)\n",
    "    \n",
    "    # Parameters\n",
    "    vocab_size = len(char_to_idx)\n",
    "    embedding_dim = 16\n",
    "    hidden_units = 50\n",
    "    epochs = 100\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Convert inputs/targets to tensors\n",
    "    inputs = np.array(inputs)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    # Reshape inputs to match the expected shape for RNN: (batch_size, timesteps, input_dim)\n",
    "    inputs = inputs.reshape((inputs.shape[0], 1))  # Adding a 'timesteps' axis\n",
    "    targets = targets.reshape((targets.shape[0], 1))  # Adding a 'timesteps' axis\n",
    "\n",
    "    # Create dataset and batch it\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
    "    dataset = dataset.batch(1, drop_remainder=True).shuffle(100)  # Batch size of 1 and shuffle\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = build_rnn_model(vocab_size, embedding_dim, hidden_units)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy')\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        for batch_inputs, batch_targets in dataset:\n",
    "            history = model.fit(batch_inputs, batch_targets, verbose=0)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {history.history['loss'][0]}\")\n",
    "\n",
    "    # Generate text\n",
    "    print(generate_text(model, \"h\", char_to_idx, idx_to_char, length=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coding from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 2.5316583343958676\n",
      "Epoch 2/100, Loss: 2.464115654074246\n",
      "Epoch 3/100, Loss: 2.3968181145447116\n",
      "Epoch 4/100, Loss: 2.329754072605854\n",
      "Epoch 5/100, Loss: 2.2629374661225383\n",
      "Epoch 6/100, Loss: 2.1964793261498614\n",
      "Epoch 7/100, Loss: 2.1306520311081742\n",
      "Epoch 8/100, Loss: 2.065925667295679\n",
      "Epoch 9/100, Loss: 2.0029440539907593\n",
      "Epoch 10/100, Loss: 1.9423643461724809\n",
      "Epoch 11/100, Loss: 1.8847049381374004\n",
      "Epoch 12/100, Loss: 1.8302838070288139\n",
      "Epoch 13/100, Loss: 1.7791072793611726\n",
      "Epoch 14/100, Loss: 1.7310137637275322\n",
      "Epoch 15/100, Loss: 1.685711713876372\n",
      "Epoch 16/100, Loss: 1.642909151028769\n",
      "Epoch 17/100, Loss: 1.6023491364546836\n",
      "Epoch 18/100, Loss: 1.563818492703086\n",
      "Epoch 19/100, Loss: 1.5271488808703897\n",
      "Epoch 20/100, Loss: 1.4922145622176797\n",
      "Epoch 21/100, Loss: 1.4588863107458878\n",
      "Epoch 22/100, Loss: 1.4270411706003165\n",
      "Epoch 23/100, Loss: 1.396568938954962\n",
      "Epoch 24/100, Loss: 1.367370590839457\n",
      "Epoch 25/100, Loss: 1.3393532389430856\n",
      "Epoch 26/100, Loss: 1.3124337351008912\n",
      "Epoch 27/100, Loss: 1.2865320882561868\n",
      "Epoch 28/100, Loss: 1.2615742626360604\n",
      "Epoch 29/100, Loss: 1.2374949872714458\n",
      "Epoch 30/100, Loss: 1.2142340467470234\n",
      "Epoch 31/100, Loss: 1.1917374716569913\n",
      "Epoch 32/100, Loss: 1.1699590142964846\n",
      "Epoch 33/100, Loss: 1.1488588835002276\n",
      "Epoch 34/100, Loss: 1.1283997446413234\n",
      "Epoch 35/100, Loss: 1.1085484038083189\n",
      "Epoch 36/100, Loss: 1.0892761211997934\n",
      "Epoch 37/100, Loss: 1.0705560793435707\n",
      "Epoch 38/100, Loss: 1.052365125954883\n",
      "Epoch 39/100, Loss: 1.034683117505207\n",
      "Epoch 40/100, Loss: 1.0174897415254014\n",
      "Epoch 41/100, Loss: 1.0007671502345514\n",
      "Epoch 42/100, Loss: 0.9844981971733097\n",
      "Epoch 43/100, Loss: 0.9686665317164096\n",
      "Epoch 44/100, Loss: 0.9532574293880287\n",
      "Epoch 45/100, Loss: 0.9382574003255473\n",
      "Epoch 46/100, Loss: 0.9236537431919885\n",
      "Epoch 47/100, Loss: 0.9094333425339393\n",
      "Epoch 48/100, Loss: 0.8955838156660214\n",
      "Epoch 49/100, Loss: 0.8820931148766414\n",
      "Epoch 50/100, Loss: 0.8689493118992588\n",
      "Epoch 51/100, Loss: 0.8561409099089813\n",
      "Epoch 52/100, Loss: 0.8436568188836523\n",
      "Epoch 53/100, Loss: 0.8314863361933135\n",
      "Epoch 54/100, Loss: 0.8196191783387674\n",
      "Epoch 55/100, Loss: 0.8080459725644029\n",
      "Epoch 56/100, Loss: 0.7967570914310953\n",
      "Epoch 57/100, Loss: 0.7857445684203658\n",
      "Epoch 58/100, Loss: 0.7749993586126984\n",
      "Epoch 59/100, Loss: 0.7645133215418416\n",
      "Epoch 60/100, Loss: 0.7542787756292794\n",
      "Epoch 61/100, Loss: 0.7442883578419168\n",
      "Epoch 62/100, Loss: 0.734534125542063\n",
      "Epoch 63/100, Loss: 0.725008425971653\n",
      "Epoch 64/100, Loss: 0.7157039598786754\n",
      "Epoch 65/100, Loss: 0.7066146347740314\n",
      "Epoch 66/100, Loss: 0.6977335649827562\n",
      "Epoch 67/100, Loss: 0.689054126796335\n",
      "Epoch 68/100, Loss: 0.6805699489158471\n",
      "Epoch 69/100, Loss: 0.672274902975786\n",
      "Epoch 70/100, Loss: 0.6641630941892669\n",
      "Epoch 71/100, Loss: 0.6562288521517509\n",
      "Epoch 72/100, Loss: 0.6484667218359845\n",
      "Epoch 73/100, Loss: 0.6408714548060244\n",
      "Epoch 74/100, Loss: 0.6334380006733153\n",
      "Epoch 75/100, Loss: 0.6261614988130271\n",
      "Epoch 76/100, Loss: 0.6190372703543903\n",
      "Epoch 77/100, Loss: 0.6120608104546894\n",
      "Epoch 78/100, Loss: 0.6052277808629083\n",
      "Epoch 79/100, Loss: 0.5985340027758191\n",
      "Epoch 80/100, Loss: 0.59197544998653\n",
      "Epoch 81/100, Loss: 0.5855482423231558\n",
      "Epoch 82/100, Loss: 0.5792486393733224\n",
      "Epoch 83/100, Loss: 0.5730730344886028\n",
      "Epoch 84/100, Loss: 0.5670179490617225\n",
      "Epoch 85/100, Loss: 0.5610800270683562\n",
      "Epoch 86/100, Loss: 0.5552560298646151\n",
      "Epoch 87/100, Loss: 0.5495428312307762\n",
      "Epoch 88/100, Loss: 0.5439374126514772\n",
      "Epoch 89/100, Loss: 0.5384368588223989\n",
      "Epoch 90/100, Loss: 0.533038353373408\n",
      "Epoch 91/100, Loss: 0.5277391747981751\n",
      "Epoch 92/100, Loss: 0.5225366925804196\n",
      "Epoch 93/100, Loss: 0.517428363507141\n",
      "Epoch 94/100, Loss: 0.5124117281594527\n",
      "Epoch 95/100, Loss: 0.5074844075719276\n",
      "Epoch 96/100, Loss: 0.5026441000517137\n",
      "Epoch 97/100, Loss: 0.4978885781490006\n",
      "Epoch 98/100, Loss: 0.4932156857708133\n",
      "Epoch 99/100, Loss: 0.4886233354304526\n",
      "Epoch 100/100, Loss: 0.4841095056252926\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # For numerical stability\n",
    "    return exp_x / exp_x.sum(axis=0)\n",
    "\n",
    "# One-hot encoding\n",
    "def one_hot_encoding(input_sequence):\n",
    "    unique_chars = sorted(set(input_sequence))\n",
    "    char_to_idx = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "    \n",
    "    one_hot_dict = {\n",
    "        char: [1 if i == char_to_idx[char] else 0 for i in range(len(unique_chars))]\n",
    "        for char in unique_chars\n",
    "    }\n",
    "    return char_to_idx, idx_to_char, one_hot_dict\n",
    "\n",
    "# Prepare sequences\n",
    "def prepare_sequences(sequence, char_to_idx):\n",
    "    inputs = sequence[:-1]\n",
    "    targets = sequence[1:]\n",
    "    return inputs, [char_to_idx[char] for char in targets]\n",
    "\n",
    "# RNN Class\n",
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Weight matrices with Xavier Initialization\n",
    "        self.wih = np.random.randn(hidden_size, input_size) * np.sqrt(2. / input_size)\n",
    "        self.whh = np.random.randn(hidden_size, hidden_size) * np.sqrt(2. / hidden_size)\n",
    "        self.who = np.random.randn(output_size, hidden_size) * np.sqrt(2. / hidden_size)\n",
    "        \n",
    "        # Biases\n",
    "        self.bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "        self.bo = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        h_next = tanh(np.dot(self.wih, x) + np.dot(self.whh, h_prev) + self.bh)\n",
    "        y = np.dot(self.who, h_next) + self.bo\n",
    "        y = softmax(y)\n",
    "        return y, h_next\n",
    "\n",
    "    def train(self, inputs, targets, one_hot_dict, epochs=100):\n",
    "        vocab_size = len(one_hot_dict)\n",
    "        loss_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            h_prev = np.zeros((self.hidden_size, 1))  # Reset hidden state\n",
    "            total_loss = 0\n",
    "            dwih, dwhh, dwho = np.zeros_like(self.wih), np.zeros_like(self.whh), np.zeros_like(self.who)\n",
    "            dbh, dbo = np.zeros_like(self.bh), np.zeros_like(self.bo)\n",
    "            xs, hs, ys, ps = {}, {}, {}, {}\n",
    "            hs[-1] = np.copy(h_prev)\n",
    "            \n",
    "            # Forward pass\n",
    "            for t in range(len(inputs)):\n",
    "                xs[t] = np.array(one_hot_dict[inputs[t]]).reshape(-1, 1)  # One-hot\n",
    "                ys[t] = np.zeros((vocab_size, 1))\n",
    "                ys[t][targets[t]] = 1  # True label\n",
    "                \n",
    "                ps[t], hs[t] = self.forward(xs[t], hs[t-1])\n",
    "                total_loss += -np.log(ps[t][targets[t], 0])  # Cross-entropy loss\n",
    "            \n",
    "            # Backward pass\n",
    "            dh_next = np.zeros_like(hs[0])\n",
    "            for t in reversed(range(len(inputs))):\n",
    "                dy = ps[t] - ys[t]  # Gradient of output\n",
    "                dwho += np.dot(dy, hs[t].T)\n",
    "                dbo += dy\n",
    "                \n",
    "                dh = np.dot(self.who.T, dy) + dh_next  # Backprop through hidden\n",
    "                dh_raw = dh * (1 - hs[t]**2)  # tanh derivative\n",
    "                dwih += np.dot(dh_raw, xs[t].T)\n",
    "                dwhh += np.dot(dh_raw, hs[t-1].T)\n",
    "                dbh += dh_raw\n",
    "                \n",
    "                dh_next = np.dot(self.whh.T, dh_raw)\n",
    "          \n",
    "            # Gradient clipping\n",
    "            for dparam in [dwih, dwhh, dwho, dbh, dbo]:\n",
    "                np.clip(dparam, -1, 1, out=dparam)\n",
    "         \n",
    "            # Update parameters\n",
    "            self.wih -= self.learning_rate * dwih\n",
    "            self.whh -= self.learning_rate * dwhh\n",
    "            self.who -= self.learning_rate * dwho\n",
    "            self.bh -= self.learning_rate * dbh\n",
    "            self.bo -= self.learning_rate * dbo\n",
    "\n",
    "            loss_history.append(total_loss / len(inputs))\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(inputs)}\")\n",
    "        \n",
    "        return loss_history\n",
    "\n",
    "# Text Generation\n",
    "def generate_text(rnn, start_char, char_to_idx, idx_to_char, one_hot_dict, length=10):\n",
    "    h_prev = np.zeros((rnn.hidden_size, 1))\n",
    "    x = np.array(one_hot_dict[start_char]).reshape(-1, 1)\n",
    "    result = start_char\n",
    "\n",
    "    for _ in range(length):\n",
    "        y, h_prev = rnn.forward(x, h_prev)\n",
    "        idx = np.argmax(y)\n",
    "        result += idx_to_char[idx]\n",
    "        x = np.array(one_hot_dict[idx_to_char[idx]]).reshape(-1, 1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_sequence = \"hello world\"\n",
    "    char_to_idx, idx_to_char, one_hot_dict = one_hot_encoding(input_sequence)\n",
    "    inputs, targets = prepare_sequences(input_sequence, char_to_idx)\n",
    "    rnn = RNN(input_size=len(one_hot_dict), hidden_size=50, output_size=len(one_hot_dict), learning_rate=0.001)\n",
    "    loss_history = rnn.train(inputs, targets, one_hot_dict, epochs=100)\n",
    "    print(generate_text(rnn, \"h\", char_to_idx, idx_to_char, one_hot_dict, length=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 2.1272165954323112\n",
      "Epoch 2/100, Loss: 2.0885484291638075\n",
      "Epoch 3/100, Loss: 2.0506197002498396\n",
      "Epoch 4/100, Loss: 2.0134290606998166\n",
      "Epoch 5/100, Loss: 1.976972717075563\n",
      "Epoch 6/100, Loss: 1.9412456915381562\n",
      "Epoch 7/100, Loss: 1.9062424345022408\n",
      "Epoch 8/100, Loss: 1.8719538961211246\n",
      "Epoch 9/100, Loss: 1.838368943432236\n",
      "Epoch 10/100, Loss: 1.8054772150883414\n",
      "Epoch 11/100, Loss: 1.7732654921031372\n",
      "Epoch 12/100, Loss: 1.7417197709080803\n",
      "Epoch 13/100, Loss: 1.7108255419879541\n",
      "Epoch 14/100, Loss: 1.680571502248358\n",
      "Epoch 15/100, Loss: 1.6509462538487027\n",
      "Epoch 16/100, Loss: 1.6219379840399695\n",
      "Epoch 17/100, Loss: 1.5935326515814212\n",
      "Epoch 18/100, Loss: 1.5657175998712753\n",
      "Epoch 19/100, Loss: 1.5384792472459052\n",
      "Epoch 20/100, Loss: 1.5118030671733569\n",
      "Epoch 21/100, Loss: 1.485674965598621\n",
      "Epoch 22/100, Loss: 1.4600821426209145\n",
      "Epoch 23/100, Loss: 1.4350125224911392\n",
      "Epoch 24/100, Loss: 1.4104554861532976\n",
      "Epoch 25/100, Loss: 1.3863992278366537\n",
      "Epoch 26/100, Loss: 1.3628338845549097\n",
      "Epoch 27/100, Loss: 1.339748598728558\n",
      "Epoch 28/100, Loss: 1.3171326424188496\n",
      "Epoch 29/100, Loss: 1.2949757712926533\n",
      "Epoch 30/100, Loss: 1.273268200926737\n",
      "Epoch 31/100, Loss: 1.2520013413166633\n",
      "Epoch 32/100, Loss: 1.2311675751452218\n",
      "Epoch 33/100, Loss: 1.210758887312936\n",
      "Epoch 34/100, Loss: 1.1907679141349756\n",
      "Epoch 35/100, Loss: 1.171186660557308\n",
      "Epoch 36/100, Loss: 1.1520076981330074\n",
      "Epoch 37/100, Loss: 1.1332261516488464\n",
      "Epoch 38/100, Loss: 1.1148346183604854\n",
      "Epoch 39/100, Loss: 1.0968262285729418\n",
      "Epoch 40/100, Loss: 1.0791960982323539\n",
      "Epoch 41/100, Loss: 1.0619372250585795\n",
      "Epoch 42/100, Loss: 1.045042725148653\n",
      "Epoch 43/100, Loss: 1.0285058153910238\n",
      "Epoch 44/100, Loss: 1.0123197981634628\n",
      "Epoch 45/100, Loss: 0.9964780483002172\n",
      "Epoch 46/100, Loss: 0.9809740022760873\n",
      "Epoch 47/100, Loss: 0.9658011495220388\n",
      "Epoch 48/100, Loss: 0.9509530257589439\n",
      "Epoch 49/100, Loss: 0.9364232082130541\n",
      "Epoch 50/100, Loss: 0.9222053125588663\n",
      "Epoch 51/100, Loss: 0.9082929914220096\n",
      "Epoch 52/100, Loss: 0.8946799342664162\n",
      "Epoch 53/100, Loss: 0.8813598684860893\n",
      "Epoch 54/100, Loss: 0.868326561521779\n",
      "Epoch 55/100, Loss: 0.8555738238264767\n",
      "Epoch 56/100, Loss: 0.8430955125102444\n",
      "Epoch 57/100, Loss: 0.8308855355040687\n",
      "Epoch 58/100, Loss: 0.8189378560936024\n",
      "Epoch 59/100, Loss: 0.8072464976863248\n",
      "Epoch 60/100, Loss: 0.7958055486893317\n",
      "Epoch 61/100, Loss: 0.7846091673891994\n",
      "Epoch 62/100, Loss: 0.7736515867397556\n",
      "Epoch 63/100, Loss: 0.762927118977789\n",
      "Epoch 64/100, Loss: 0.752430160000419\n",
      "Epoch 65/100, Loss: 0.7421551934508561\n",
      "Epoch 66/100, Loss: 0.7320967944713491\n",
      "Epoch 67/100, Loss: 0.7222496330931845\n",
      "Epoch 68/100, Loss: 0.7126084772435612\n",
      "Epoch 69/100, Loss: 0.7031681953579758\n",
      "Epoch 70/100, Loss: 0.693923758594432\n",
      "Epoch 71/100, Loss: 0.6848702426523414\n",
      "Epoch 72/100, Loss: 0.676002829204482\n",
      "Epoch 73/100, Loss: 0.667316806954872\n",
      "Epoch 74/100, Loss: 0.6588075723389986\n",
      "Epoch 75/100, Loss: 0.6504706298855788\n",
      "Epoch 76/100, Loss: 0.6423015922610456\n",
      "Epoch 77/100, Loss: 0.634296180019287\n",
      "Epoch 78/100, Loss: 0.6264502210799671\n",
      "Epoch 79/100, Loss: 0.6187596499590552\n",
      "Epoch 80/100, Loss: 0.6112205067751035\n",
      "Epoch 81/100, Loss: 0.603828936054383\n",
      "Epoch 82/100, Loss: 0.5965811853573073\n",
      "Epoch 83/100, Loss: 0.5894736037476793\n",
      "Epoch 84/100, Loss: 0.5825026401252495\n",
      "Epoch 85/100, Loss: 0.5756648414409214\n",
      "Epoch 86/100, Loss: 0.5689568508127101\n",
      "Epoch 87/100, Loss: 0.5623754055592947\n",
      "Epoch 88/100, Loss: 0.5559173351667273\n",
      "Epoch 89/100, Loss: 0.5495795592025846\n",
      "Epoch 90/100, Loss: 0.5433590851906105\n",
      "Epoch 91/100, Loss: 0.5372530064576844\n",
      "Epoch 92/100, Loss: 0.5312584999638036\n",
      "Epoch 93/100, Loss: 0.5253728241246562\n",
      "Epoch 94/100, Loss: 0.5195933166353419\n",
      "Epoch 95/100, Loss: 0.5139173923028041\n",
      "Epoch 96/100, Loss: 0.5083425408936563\n",
      "Epoch 97/100, Loss: 0.502866325003227\n",
      "Epoch 98/100, Loss: 0.49748637795088607\n",
      "Epoch 99/100, Loss: 0.49220040170600515\n",
      "Epoch 100/100, Loss: 0.48700616484825515\n",
      "worldo\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_sequence = \"world\"\n",
    "    char_to_idx, idx_to_char, one_hot_dict = one_hot_encoding(input_sequence)\n",
    "    inputs, targets = prepare_sequences(input_sequence, char_to_idx)\n",
    "    rnn = RNN(input_size=len(one_hot_dict), hidden_size=50, output_size=len(one_hot_dict), learning_rate=0.001)\n",
    "    loss_history = rnn.train(inputs, targets, one_hot_dict, epochs=100)\n",
    "    print(generate_text(rnn, \"w\", char_to_idx, idx_to_char, one_hot_dict, length=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 2.155918115140609\n",
      "Epoch 2/100, Loss: 2.1083164614807512\n",
      "Epoch 3/100, Loss: 2.0615253038967634\n",
      "Epoch 4/100, Loss: 2.0155844447840248\n",
      "Epoch 5/100, Loss: 1.9705295873959177\n",
      "Epoch 6/100, Loss: 1.9263850711016877\n",
      "Epoch 7/100, Loss: 1.8831666320906024\n",
      "Epoch 8/100, Loss: 1.8408844522124166\n",
      "Epoch 9/100, Loss: 1.7995391085285588\n",
      "Epoch 10/100, Loss: 1.75912871638921\n",
      "Epoch 11/100, Loss: 1.719647615191402\n",
      "Epoch 12/100, Loss: 1.681079772265026\n",
      "Epoch 13/100, Loss: 1.6434056594637552\n",
      "Epoch 14/100, Loss: 1.6066048479796786\n",
      "Epoch 15/100, Loss: 1.570655647101998\n",
      "Epoch 16/100, Loss: 1.5355365307074407\n",
      "Epoch 17/100, Loss: 1.5012266319292835\n",
      "Epoch 18/100, Loss: 1.4677060670712216\n",
      "Epoch 19/100, Loss: 1.4349605029898675\n",
      "Epoch 20/100, Loss: 1.402974186283065\n",
      "Epoch 21/100, Loss: 1.3717366099083659\n",
      "Epoch 22/100, Loss: 1.3412375885128103\n",
      "Epoch 23/100, Loss: 1.3114650298418438\n",
      "Epoch 24/100, Loss: 1.2824112062442234\n",
      "Epoch 25/100, Loss: 1.2540646590414553\n",
      "Epoch 26/100, Loss: 1.2264126873398653\n",
      "Epoch 27/100, Loss: 1.199443343886987\n",
      "Epoch 28/100, Loss: 1.173143469115339\n",
      "Epoch 29/100, Loss: 1.1475012977436747\n",
      "Epoch 30/100, Loss: 1.1225058202177107\n",
      "Epoch 31/100, Loss: 1.0981460778743557\n",
      "Epoch 32/100, Loss: 1.0744107881306124\n",
      "Epoch 33/100, Loss: 1.0512871939067796\n",
      "Epoch 34/100, Loss: 1.028762942603958\n",
      "Epoch 35/100, Loss: 1.0068281669751238\n",
      "Epoch 36/100, Loss: 0.9854729790549933\n",
      "Epoch 37/100, Loss: 0.964684256798503\n",
      "Epoch 38/100, Loss: 0.9444497054884105\n",
      "Epoch 39/100, Loss: 0.9247587724451851\n",
      "Epoch 40/100, Loss: 0.9055981745232164\n",
      "Epoch 41/100, Loss: 0.8869561632892673\n",
      "Epoch 42/100, Loss: 0.8688198719526549\n",
      "Epoch 43/100, Loss: 0.851176436017915\n",
      "Epoch 44/100, Loss: 0.8340150764292577\n",
      "Epoch 45/100, Loss: 0.8173228967858157\n",
      "Epoch 46/100, Loss: 0.801087553758566\n",
      "Epoch 47/100, Loss: 0.7852980277978081\n",
      "Epoch 48/100, Loss: 0.769941900005363\n",
      "Epoch 49/100, Loss: 0.7550069529638439\n",
      "Epoch 50/100, Loss: 0.7404811836333051\n",
      "Epoch 51/100, Loss: 0.7263528144514605\n",
      "Epoch 52/100, Loss: 0.7126103026997542\n",
      "Epoch 53/100, Loss: 0.6992436163556057\n",
      "Epoch 54/100, Loss: 0.6862428801875683\n",
      "Epoch 55/100, Loss: 0.6735999196015998\n",
      "Epoch 56/100, Loss: 0.6613039504519066\n",
      "Epoch 57/100, Loss: 0.6493444501342848\n",
      "Epoch 58/100, Loss: 0.6377111595447473\n",
      "Epoch 59/100, Loss: 0.6263940840050782\n",
      "Epoch 60/100, Loss: 0.6153834932633968\n",
      "Epoch 61/100, Loss: 0.6046699206713213\n",
      "Epoch 62/100, Loss: 0.5942441616322556\n",
      "Epoch 63/100, Loss: 0.5840972714080322\n",
      "Epoch 64/100, Loss: 0.5742205623638627\n",
      "Epoch 65/100, Loss: 0.5646056007244995\n",
      "Epoch 66/100, Loss: 0.5552442029077852\n",
      "Epoch 67/100, Loss: 0.5461284314954673\n",
      "Epoch 68/100, Loss: 0.5372505908953037\n",
      "Epoch 69/100, Loss: 0.5286032227431238\n",
      "Epoch 70/100, Loss: 0.5201791010885999\n",
      "Epoch 71/100, Loss: 0.5119712274040416\n",
      "Epoch 72/100, Loss: 0.5039728254514972\n",
      "Epoch 73/100, Loss: 0.49617733603981645\n",
      "Epoch 74/100, Loss: 0.48857841170005095\n",
      "Epoch 75/100, Loss: 0.4811699113046188\n",
      "Epoch 76/100, Loss: 0.47394589465299847\n",
      "Epoch 77/100, Loss: 0.46690061704430785\n",
      "Epoch 78/100, Loss: 0.4600285238549491\n",
      "Epoch 79/100, Loss: 0.45332424513752956\n",
      "Epoch 80/100, Loss: 0.4467825902554696\n",
      "Epoch 81/100, Loss: 0.4403985425660784\n",
      "Epoch 82/100, Loss: 0.4341672541633867\n",
      "Epoch 83/100, Loss: 0.42808404069065764\n",
      "Epoch 84/100, Loss: 0.42214437623125334\n",
      "Epoch 85/100, Loss: 0.41634388828538654\n",
      "Epoch 86/100, Loss: 0.4106783528392329\n",
      "Epoch 87/100, Loss: 0.4051436895319211\n",
      "Epoch 88/100, Loss: 0.39973595692503017\n",
      "Epoch 89/100, Loss: 0.39445134787840963\n",
      "Epoch 90/100, Loss: 0.3892861850354038\n",
      "Epoch 91/100, Loss: 0.3842369164198729\n",
      "Epoch 92/100, Loss: 0.37930011114679574\n",
      "Epoch 93/100, Loss: 0.3744724552476667\n",
      "Epoch 94/100, Loss: 0.36975074761139504\n",
      "Epoch 95/100, Loss: 0.36513189604095336\n",
      "Epoch 96/100, Loss: 0.36061291342560325\n",
      "Epoch 97/100, Loss: 0.35619091402816055\n",
      "Epoch 98/100, Loss: 0.35186310988642366\n",
      "Epoch 99/100, Loss: 0.3476268073276023\n",
      "Epoch 100/100, Loss: 0.3434794035943203\n",
      "applee\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_sequence = \"apple\"\n",
    "    char_to_idx, idx_to_char, one_hot_dict = one_hot_encoding(input_sequence)\n",
    "    inputs, targets = prepare_sequences(input_sequence, char_to_idx)\n",
    "    rnn = RNN(input_size=len(one_hot_dict), hidden_size=50, output_size=len(one_hot_dict), learning_rate=0.001)\n",
    "    loss_history = rnn.train(inputs, targets, one_hot_dict, epochs=100)\n",
    "    print(generate_text(rnn, \"a\", char_to_idx, idx_to_char, one_hot_dict, length=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 2.088831211144503\n",
      "Epoch 2/100, Loss: 2.0487816029114994\n",
      "Epoch 3/100, Loss: 2.009812342942497\n",
      "Epoch 4/100, Loss: 1.9718749207335076\n",
      "Epoch 5/100, Loss: 1.934918533780427\n",
      "Epoch 6/100, Loss: 1.898897059314382\n",
      "Epoch 7/100, Loss: 1.863759827459249\n",
      "Epoch 8/100, Loss: 1.8294553291868838\n",
      "Epoch 9/100, Loss: 1.7959371145798149\n",
      "Epoch 10/100, Loss: 1.7631653677037515\n",
      "Epoch 11/100, Loss: 1.7311005262602486\n",
      "Epoch 12/100, Loss: 1.6997028209052583\n",
      "Epoch 13/100, Loss: 1.6689365773431253\n",
      "Epoch 14/100, Loss: 1.6387719904035811\n",
      "Epoch 15/100, Loss: 1.609182003168873\n",
      "Epoch 16/100, Loss: 1.5801421308059596\n",
      "Epoch 17/100, Loss: 1.5516313225200626\n",
      "Epoch 18/100, Loss: 1.5236341276565772\n",
      "Epoch 19/100, Loss: 1.4961367794835714\n",
      "Epoch 20/100, Loss: 1.469126544810782\n",
      "Epoch 21/100, Loss: 1.4425927321840952\n",
      "Epoch 22/100, Loss: 1.4165275326574558\n",
      "Epoch 23/100, Loss: 1.390924701216705\n",
      "Epoch 24/100, Loss: 1.3657816050033738\n",
      "Epoch 25/100, Loss: 1.3410965533290913\n",
      "Epoch 26/100, Loss: 1.3168662707949599\n",
      "Epoch 27/100, Loss: 1.2930883716859642\n",
      "Epoch 28/100, Loss: 1.2697627172343235\n",
      "Epoch 29/100, Loss: 1.2468887237431647\n",
      "Epoch 30/100, Loss: 1.2244654563188848\n",
      "Epoch 31/100, Loss: 1.2024921684707128\n",
      "Epoch 32/100, Loss: 1.180968157915732\n",
      "Epoch 33/100, Loss: 1.1598926367382334\n",
      "Epoch 34/100, Loss: 1.1392646164893674\n",
      "Epoch 35/100, Loss: 1.1190836626423657\n",
      "Epoch 36/100, Loss: 1.0993489743044154\n",
      "Epoch 37/100, Loss: 1.0800583966541022\n",
      "Epoch 38/100, Loss: 1.061209266142586\n",
      "Epoch 39/100, Loss: 1.0428009288605995\n",
      "Epoch 40/100, Loss: 1.0248320885895168\n",
      "Epoch 41/100, Loss: 1.0072981212986565\n",
      "Epoch 42/100, Loss: 0.9901938273197979\n",
      "Epoch 43/100, Loss: 0.9735134473510767\n",
      "Epoch 44/100, Loss: 0.95725068810172\n",
      "Epoch 45/100, Loss: 0.9413987559629122\n",
      "Epoch 46/100, Loss: 0.9259503970879669\n",
      "Epoch 47/100, Loss: 0.9108979423115027\n",
      "Epoch 48/100, Loss: 0.8962333554248498\n",
      "Epoch 49/100, Loss: 0.8819482834444973\n",
      "Epoch 50/100, Loss: 0.8680341076529725\n",
      "Epoch 51/100, Loss: 0.8544819943481586\n",
      "Epoch 52/100, Loss: 0.8412829443998013\n",
      "Epoch 53/100, Loss: 0.828427840873939\n",
      "Epoch 54/100, Loss: 0.8159074941416824\n",
      "Epoch 55/100, Loss: 0.803712684033999\n",
      "Epoch 56/100, Loss: 0.791834198735992\n",
      "Epoch 57/100, Loss: 0.7802628702308512\n",
      "Epoch 58/100, Loss: 0.7689896062043796\n",
      "Epoch 59/100, Loss: 0.7580054184057669\n",
      "Epoch 60/100, Loss: 0.7473014475296711\n",
      "Epoch 61/100, Loss: 0.736868984739725\n",
      "Epoch 62/100, Loss: 0.7266994899956011\n",
      "Epoch 63/100, Loss: 0.7167846073762416\n",
      "Epoch 64/100, Loss: 0.7071161776123136\n",
      "Epoch 65/100, Loss: 0.6976862480529145\n",
      "Epoch 66/100, Loss: 0.6884870802964751\n",
      "Epoch 67/100, Loss: 0.6795111557150421\n",
      "Epoch 68/100, Loss: 0.6707511790958618\n",
      "Epoch 69/100, Loss: 0.662200080615529\n",
      "Epoch 70/100, Loss: 0.6538510163508067\n",
      "Epoch 71/100, Loss: 0.6456973675173683\n",
      "Epoch 72/100, Loss: 0.6377327386138061\n",
      "Epoch 73/100, Loss: 0.6299509546338186\n",
      "Epoch 74/100, Loss: 0.6223460574949675\n",
      "Epoch 75/100, Loss: 0.6149123018181031\n",
      "Epoch 76/100, Loss: 0.6076441501777323\n",
      "Epoch 77/100, Loss: 0.6005362679304723\n",
      "Epoch 78/100, Loss: 0.5935835177163324\n",
      "Epoch 79/100, Loss: 0.5867809537160812\n",
      "Epoch 80/100, Loss: 0.5801238157373232\n",
      "Epoch 81/100, Loss: 0.5736075231922091\n",
      "Epoch 82/100, Loss: 0.5672276690208937\n",
      "Epoch 83/100, Loss: 0.560980013606905\n",
      "Epoch 84/100, Loss: 0.5548604787234771\n",
      "Epoch 85/100, Loss: 0.548865141543554\n",
      "Epoch 86/100, Loss: 0.5429902287405564\n",
      "Epoch 87/100, Loss: 0.537232110702055\n",
      "Epoch 88/100, Loss: 0.5315872958741594\n",
      "Epoch 89/100, Loss: 0.5260524252506473\n",
      "Epoch 90/100, Loss: 0.5206242670175827\n",
      "Epoch 91/100, Loss: 0.5152997113613449\n",
      "Epoch 92/100, Loss: 0.510075765445564\n",
      "Epoch 93/100, Loss: 0.5049495485603928\n",
      "Epoch 94/100, Loss: 0.4999182874457973\n",
      "Epoch 95/100, Loss: 0.4949793117890738\n",
      "Epoch 96/100, Loss: 0.49013004989558145\n",
      "Epoch 97/100, Loss: 0.48536802453066263\n",
      "Epoch 98/100, Loss: 0.4806908489299011\n",
      "Epoch 99/100, Loss: 0.4760962229742057\n",
      "Epoch 100/100, Loss: 0.47158192952567857\n",
      "indigo\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_sequence = \"indigo\"\n",
    "    char_to_idx, idx_to_char, one_hot_dict = one_hot_encoding(input_sequence)\n",
    "    inputs, targets = prepare_sequences(input_sequence, char_to_idx)\n",
    "    rnn = RNN(input_size=len(one_hot_dict), hidden_size=50, output_size=len(one_hot_dict), learning_rate=0.001)\n",
    "    loss_history = rnn.train(inputs, targets, one_hot_dict, epochs=100)\n",
    "    print(generate_text(rnn, \"i\", char_to_idx, idx_to_char, one_hot_dict, length=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
